{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa3951f-83fd-43a4-b972-59d41861dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9453c7-75a4-4452-b78e-5115f7f93061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape: (60000, 784)\n",
      "train_Y (one-hot) shape: (60000, 10)\n",
      "test_X shape: (10000, 784)\n",
      "test_Y (one-hot) shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "                    transform=lambda x: np.array(x).flatten(),\n",
    "                    download=True,\n",
    "                    train=is_train)\n",
    "    \n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "    \n",
    "    mnist_data = np.array(mnist_data, dtype='float64')\n",
    "    mnist_labels = np.array(mnist_labels)\n",
    "    \n",
    "    mnist_labels = one_hot_encode(mnist_labels, num_classes=10)\n",
    "    \n",
    "    return mnist_data, mnist_labels\n",
    "    \n",
    "train_X, train_Y = download_mnist(True)\n",
    "test_X, test_Y = download_mnist(False)\n",
    "\n",
    "print(f\"train_X shape: {train_X.shape}\")\n",
    "print(f\"train_Y (one-hot) shape: {train_Y.shape}\")\n",
    "print(f\"test_X shape: {test_X.shape}\")\n",
    "print(f\"test_Y (one-hot) shape: {test_Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eeba5c-d2bb-4e7e-bafe-a5357a0f3248",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd4b2c5-82fd-444e-9610-cfe000a8cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X /= 255.0\n",
    "test_X /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3469355e-5464-4ed5-b53a-4355f94015a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Min value: {min([min(t) for t in train_X])}\")\n",
    "# print(f\"Max value: {max([max(t) for t in train_X])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf91485-b4df-475a-8727-5b238352536b",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad1f9378-abd9-4071-ab1b-ec47adbf971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_X.shape[1]\n",
    "output_size = train_Y.shape[1]\n",
    "batch_size = 64\n",
    "learning_rate = 0.07\n",
    "epochs = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a82ab-9283-4705-8bac-54e53da6aabb",
   "metadata": {},
   "source": [
    "## Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2e832a-24ab-4331-a773-cd7f42633a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(X, Y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        Y_batch = Y[i:i+batch_size]\n",
    "        yield X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55348321-9388-40bc-8e26-7d37e9d0dac7",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b733ac33-963b-4e49-aeec-f8abc7afb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax_derivative(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d210c-af5b-49c4-a8b5-8450c082d41b",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61c8bd4-2467-47b1-978c-2d6c06c1c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(batch_Y, Y_pred):\n",
    "    return -np.sum(batch_Y * np.log(Y_pred + 1e-10)) / len(batch_Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26534647-92fc-4228-8116-63b84ab434f1",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a1ece1c-e840-4cde-8190-fb05671573a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = np.random.randn(input_size, output_size) * 0.1\n",
    "# b = np.zeros(output_size)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for batch_X, batch_Y in generate_batches(train_X, train_Y, batch_size):\n",
    "#         #print(f\"Batch_X shape:{batch_X.shape}\")\n",
    "#         #print(f\"Weights shape: {W.shape}\")\n",
    "#         #print(f\"Bias shape: {b.shape}\")\n",
    "        \n",
    "#         Z = np.dot(batch_X, W) + b \n",
    "#         #print(f\"Z shape: {Z.shape}\")\n",
    "#         Y_pred = softmax(Z)\n",
    "#         #print(f\"Y_pred shape: {Y_pred.shape}\")\n",
    "#         Y_hat = np.argmax(Y_pred, axis=1)\n",
    "#         #print(f\"Prediction shape: {Y_hat.shape}\")\n",
    "\n",
    "#         loss = -np.sum(batch_Y * np.log(Y_pred + 1e-10)) / batch_size  # Cross-entropy loss\n",
    "        \n",
    "#         # Backprop\n",
    "#         error = batch_Y - Y_pred\n",
    "\n",
    "#         dW = np.dot(batch_X.T, error) / batch_size\n",
    "#         db = np.sum(error, axis=0) / batch_size\n",
    "\n",
    "#         W += learning_rate * dW\n",
    "#         b += learning_rate * db\n",
    "        \n",
    "#     print(f\"Epoch {epoch+1}/{epochs} Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bb91b43-9514-4f37-b747-80bc2cb06753",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_func_dict = {\n",
    "    'softmax': softmax,\n",
    "    'relu': relu\n",
    "}\n",
    "\n",
    "loss_func_dict = {\n",
    "    \"crossentropy\": cross_entropy\n",
    "}\n",
    "\n",
    "\n",
    "activation_deriv_func_dict = {\n",
    "    'relu': relu_derivative,\n",
    "    'softmax': softmax_derivative\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad744f51-2a9e-43b8-83ac-32d151cd3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerDense:\n",
    "    def __init__(self, shape, activation='softmax'):\n",
    "        n_inputs, n_neurons = shape[0], shape[1]\n",
    "        self.weights = 0.15 * np.random.randn(n_neurons, n_inputs).T\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.activation = activation_func_dict[activation]\n",
    "        self.activation_deriv = activation_deriv_func_dict[activation]\n",
    "\n",
    "        # Unitialized\n",
    "        self.inputs = None\n",
    "        self.output = None\n",
    "        self.d_weights = None\n",
    "        self.d_biases = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        Z = np.dot(inputs, self.weights) + self.biases\n",
    "        self.output = self.activation(Z)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        d_activation = d_output * self.activation_deriv(self.output)\n",
    "\n",
    "        # Gradients\n",
    "        batch_size = self.inputs.shape[0]\n",
    "        self.d_weights = np.dot(self.inputs.T, d_activation) / batch_size\n",
    "        self.d_biases = np.sum(d_activation, axis=0, keepdims=True) / batch_size\n",
    "        d_input = np.dot(d_activation, self.weights.T)\n",
    "\n",
    "        return d_input\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, rate=0.1):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        if training:\n",
    "            # 1 with probability (1 - rate), 0 with probability rate\n",
    "            self.mask = np.random.binomial(1, 1 - self.rate, size=inputs.shape) / (1 - self.rate)\n",
    "            return inputs * self.mask\n",
    "        else:\n",
    "            # inference, pass the input unchanged\n",
    "            return inputs\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # dropout mask during the backward pass\n",
    "        return d_output * self.mask\n",
    "        \n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, debug=False):\n",
    "        self.layers = layers\n",
    "        self.debug = debug\n",
    "\n",
    "        if debug:\n",
    "            for i, layer in enumerate(layers):\n",
    "                print(f\"Layer_{i} shape: {layer.weights.shape}\")\n",
    "        \n",
    "    def __forward(self, batch_X, training=True):\n",
    "        input_values = batch_X\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                output_values = layer.forward(input_values, training=training)\n",
    "            else:\n",
    "                output_values = layer.forward(input_values)\n",
    "            input_values = output_values\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Output:\\n{output_values}\")\n",
    "    \n",
    "        return output_values\n",
    "\n",
    "\n",
    "    def __backward(self, batch_Y, Y_pred):\n",
    "        d_output = Y_pred - batch_Y  \n",
    "        # Cross-entropy loss derivative with softmax output\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            d_output = layer.backward(d_output)\n",
    "\n",
    "    def train(self, train_X, train_Y, epochs, loss_type=\"crossentropy\", learning_rate=0.01):\n",
    "        print(\"Starting the crazy stuff\")\n",
    "        loss_func = loss_func_dict[loss_type]\n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_Y in generate_batches(train_X, train_Y, batch_size):\n",
    "                Y_pred = self.__forward(batch_X, training=True)\n",
    "                \n",
    "                loss = loss_func(batch_Y, Y_pred)\n",
    "                error = batch_Y - Y_pred\n",
    "\n",
    "                self.__backward(batch_Y, Y_pred)\n",
    "\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, Dropout):\n",
    "                        continue\n",
    "                    \n",
    "                    layer.weights -= learning_rate * layer.d_weights\n",
    "                    layer.biases -= learning_rate * layer.d_biases\n",
    "                \n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def evaluate(self, test_X, test_Y):\n",
    "        y_test_pred = model.__forward(test_X, training=False)\n",
    "    \n",
    "        y_test_hat = np.argmax(y_test_pred, axis=1)\n",
    "    \n",
    "        correct_predictions = np.sum(y_test_hat == np.argmax(test_Y, axis=1))\n",
    "        accuracy = correct_predictions / test_Y.shape[0]\n",
    "        \n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69832736-b46b-41fb-ae18-4c20007ac15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer1 = LayerDense((4, 5))\n",
    "# layer2 = LayerDense((5, 2))\n",
    "\n",
    "# print(f\"Weights: (one line is the weights for one neuron):\\n {layer1.weights.T}\")\n",
    "\n",
    "# print(f\"Biases for this layer: {layer1.biases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f34424b9-3f45-41f5-938b-d9c21aeec0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork([\n",
    "        LayerDense((input_size, 100), activation='relu'),\n",
    "        Dropout(rate=0.33),\n",
    "        LayerDense((100, output_size), activation='softmax'),\n",
    "    ], debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b41bf185-b3a7-4e28-9717-823f413d1353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the crazy stuff\n",
      "Epoch 1, Loss: 0.7185\n",
      "Epoch 2, Loss: 0.3452\n",
      "Epoch 3, Loss: 0.3505\n",
      "Epoch 4, Loss: 0.2063\n",
      "Epoch 5, Loss: 0.2340\n",
      "Epoch 6, Loss: 0.2236\n",
      "Epoch 7, Loss: 0.2503\n",
      "Epoch 8, Loss: 0.1055\n",
      "Epoch 9, Loss: 0.1243\n",
      "Epoch 10, Loss: 0.1457\n",
      "Epoch 11, Loss: 0.1165\n",
      "Epoch 12, Loss: 0.1298\n",
      "Epoch 13, Loss: 0.1334\n",
      "Epoch 14, Loss: 0.0999\n",
      "Epoch 15, Loss: 0.1112\n",
      "Epoch 16, Loss: 0.1117\n",
      "Epoch 17, Loss: 0.1447\n",
      "Epoch 18, Loss: 0.0569\n",
      "Epoch 19, Loss: 0.1050\n",
      "Epoch 20, Loss: 0.0808\n",
      "Epoch 21, Loss: 0.0903\n",
      "Epoch 22, Loss: 0.0648\n",
      "Epoch 23, Loss: 0.0983\n",
      "Epoch 24, Loss: 0.0629\n",
      "Epoch 25, Loss: 0.0883\n",
      "Epoch 26, Loss: 0.0679\n",
      "Epoch 27, Loss: 0.0701\n",
      "Epoch 28, Loss: 0.0700\n",
      "Epoch 29, Loss: 0.0602\n",
      "Epoch 30, Loss: 0.0787\n",
      "Epoch 31, Loss: 0.0562\n",
      "Epoch 32, Loss: 0.1091\n",
      "Epoch 33, Loss: 0.0376\n",
      "Epoch 34, Loss: 0.0543\n",
      "Epoch 35, Loss: 0.0783\n",
      "Epoch 36, Loss: 0.0383\n",
      "Epoch 37, Loss: 0.0570\n",
      "Epoch 38, Loss: 0.0440\n",
      "Epoch 39, Loss: 0.0636\n",
      "Epoch 40, Loss: 0.0738\n",
      "Epoch 41, Loss: 0.0541\n",
      "Epoch 42, Loss: 0.0579\n",
      "Epoch 43, Loss: 0.0427\n",
      "Epoch 44, Loss: 0.0456\n",
      "Epoch 45, Loss: 0.0686\n",
      "Epoch 46, Loss: 0.0361\n",
      "Epoch 47, Loss: 0.0339\n",
      "Epoch 48, Loss: 0.0566\n",
      "Epoch 49, Loss: 0.0290\n",
      "Epoch 50, Loss: 0.0282\n",
      "Epoch 51, Loss: 0.0286\n",
      "Epoch 52, Loss: 0.0475\n",
      "Epoch 53, Loss: 0.0258\n",
      "Epoch 54, Loss: 0.0400\n",
      "Epoch 55, Loss: 0.0357\n",
      "Epoch 56, Loss: 0.0377\n",
      "Epoch 57, Loss: 0.0392\n",
      "Epoch 58, Loss: 0.0334\n",
      "Epoch 59, Loss: 0.0440\n",
      "Epoch 60, Loss: 0.0352\n",
      "Epoch 61, Loss: 0.0368\n",
      "Epoch 62, Loss: 0.0181\n",
      "Epoch 63, Loss: 0.0258\n",
      "Epoch 64, Loss: 0.0399\n",
      "Epoch 65, Loss: 0.0170\n",
      "Epoch 66, Loss: 0.0294\n",
      "Epoch 67, Loss: 0.0324\n",
      "Epoch 68, Loss: 0.0207\n",
      "Epoch 69, Loss: 0.0323\n",
      "Epoch 70, Loss: 0.0336\n",
      "Epoch 71, Loss: 0.0180\n",
      "Epoch 72, Loss: 0.0457\n",
      "Epoch 73, Loss: 0.0159\n",
      "Epoch 74, Loss: 0.0488\n",
      "Epoch 75, Loss: 0.0736\n",
      "Training completed in 91.12 seconds\n",
      "Test Accuracy: 0.9658\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.train(train_X, train_Y, epochs=epochs, learning_rate=learning_rate)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training completed in {(end_time-start_time):.2f} seconds\")\n",
    "model.evaluate(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0252329-6665-45b2-81cf-428ad745504a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
